{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5255b5cf0aea4175bf16be3814c78f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d232a053f764e42abc089a35085ab34",
              "IPY_MODEL_f532c926b6ac446fbdd90caa0e4c5486",
              "IPY_MODEL_9c495429176c49b19f489b3157314a24"
            ],
            "layout": "IPY_MODEL_01d390d86eeb4d85916c9a0dd8ea6997"
          }
        },
        "8d232a053f764e42abc089a35085ab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6205e42a2f484e34ac932beab150449b",
            "placeholder": "​",
            "style": "IPY_MODEL_7e6082dbc7bb47b7a81be5fc4002d149",
            "value": "Loading weights: 100%"
          }
        },
        "f532c926b6ac446fbdd90caa0e4c5486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f5fc161f4943a2b3321fd4aee60b22",
            "max": 264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4541e12374044286b45b93bffad8cf22",
            "value": 264
          }
        },
        "9c495429176c49b19f489b3157314a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cedd601f35c54977adf90966face14b8",
            "placeholder": "​",
            "style": "IPY_MODEL_63ada94a93c74e71ad7a50025f9966b8",
            "value": " 264/264 [00:05&lt;00:00, 241.41it/s, Materializing param=model.shared.weight]"
          }
        },
        "01d390d86eeb4d85916c9a0dd8ea6997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6205e42a2f484e34ac932beab150449b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6082dbc7bb47b7a81be5fc4002d149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0f5fc161f4943a2b3321fd4aee60b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4541e12374044286b45b93bffad8cf22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cedd601f35c54977adf90966face14b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ada94a93c74e71ad7a50025f9966b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0qhYuOVZ_qc",
        "outputId": "b2f5e145-3eba-4d0f-a0c7-25e26a358b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/3.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/3.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hpackages are installed.\n"
          ]
        }
      ],
      "source": [
        "# necessary libraries\n",
        "#transformers & datasets: To load the IndicBART model and manage the data\n",
        "# torch: The deep learning backend (PyTorch)\n",
        "# sentencepiece: Required for the IndicBART tokenizer to handle Hindi characters\n",
        "# accelerate: To speed up training and handle GPU memory management\n",
        "# jiwer, evaluate, sacrebleu: To calculate performance metrics like WER (Word Error Rate)\n",
        "!pip install transformers datasets torch sentencepiece accelerate jiwer evaluate sacrebleu -q\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" #This prevents the trainer from asking for an API key and keeps the output clean\n",
        "\n",
        "print(\"packages are installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# 1. Mount Drive (if not already done)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#\n",
        "model_path = \"/content/drive/MyDrive/Project_model\"\n",
        "\n",
        "#\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Folder found! Loading model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True).to(\"cuda\")\n",
        "    print(\"Model loaded and ready on GPU!\")\n",
        "else:\n",
        "    print(f\"Error: Folder NOT found at {model_path}\")\n",
        "    print(\"Please check Drive folder name manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "5255b5cf0aea4175bf16be3814c78f76",
            "8d232a053f764e42abc089a35085ab34",
            "f532c926b6ac446fbdd90caa0e4c5486",
            "9c495429176c49b19f489b3157314a24",
            "01d390d86eeb4d85916c9a0dd8ea6997",
            "6205e42a2f484e34ac932beab150449b",
            "7e6082dbc7bb47b7a81be5fc4002d149",
            "f0f5fc161f4943a2b3321fd4aee60b22",
            "4541e12374044286b45b93bffad8cf22",
            "cedd601f35c54977adf90966face14b8",
            "63ada94a93c74e71ad7a50025f9966b8"
          ]
        },
        "id": "Db6_GLDKaznd",
        "outputId": "bded2140-fdad-439b-94de-d67a92dace55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Folder found! Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/264 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5255b5cf0aea4175bf16be3814c78f76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and ready on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def quick_test_inference(noisy_text):\n",
        "    model.eval()\n",
        "\n",
        "    # Input format: <Noisy> </s> <2hi>\n",
        "    input_text = f\"{noisy_text} </s> <2hi>\"\n",
        "\n",
        "    # ADDED: return_token_type_ids=False to fix the ValueError\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        return_token_type_ids=False\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Token ID for the Hindi start tag\n",
        "    hindi_tag_id = tokenizer.convert_tokens_to_ids(\"<2hi>\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            num_beams=5,\n",
        "            decoder_start_token_id=hindi_tag_id,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up any leftover language tags\n",
        "    clean_text = decoded_text.replace(\"<2hi>\", \"\").strip()\n",
        "    return clean_text\n",
        "\n",
        "# --- THE TEST RUN ---\n",
        "test_inputs = [\n",
        "    \"में जानता हु की में वहा नहीं था\",        # Test 1 (Basic Punctuation)\n",
        "    \"ये काम मुझे समझ नही आ रहा हे\",          # Test 10 (Grammar + Comma/Stop)\n",
        "    \"नमस्ते आप बहुत दिनो बाद मिले\",           # Test 7 (Ending Marker)\n",
        "    \"कल छुट्टी है क्या तुम घर आओगे\",          # Question Marker Test\n",
        "]\n",
        "\n",
        "print(\" RUNNING POST-TRAINING TEST:\\n\" + \"=\"*40)\n",
        "for inp in test_inputs:\n",
        "    output = quick_test_inference(inp)\n",
        "    print(f\"INPUT : {inp}\")\n",
        "    print(f\"OUTPUT: {output}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWZs7fcya792",
        "outputId": "30ad42c8-ce53-4d69-e777-7633561255d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RUNNING POST-TRAINING TEST:\n",
            "========================================\n",
            "INPUT : में जानता हु की में वहा नहीं था\n",
            "OUTPUT: म जनत ह क म वह नह थ।\n",
            "----------------------------------------\n",
            "INPUT : ये काम मुझे समझ नही आ रहा हे\n",
            "OUTPUT: य कम मझ समझ नह आ रह।\n",
            "----------------------------------------\n",
            "INPUT : नमस्ते आप बहुत दिनो बाद मिले\n",
            "OUTPUT: नमसत! आप बहत दन-दन बद मल।\n",
            "----------------------------------------\n",
            "INPUT : कल छुट्टी है क्या तुम घर आओगे\n",
            "OUTPUT: कल छटट! ह! कय तम घर आओग।\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "# 1. Function Definition\n",
        "def normalize_asr_strict(noisy_text):\n",
        "    model.eval()\n",
        "\n",
        "    # Input format: <Noisy> </s> <2hi>\n",
        "    input_text = f\"{noisy_text} </s> <2hi>\"\n",
        "\n",
        "    # Tokenizing with return_token_type_ids=False for compatibility\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        return_token_type_ids=False\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Hindi start tag ID\n",
        "    hindi_tag_id = tokenizer.convert_tokens_to_ids(\"<2hi>\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            num_beams=5,\n",
        "            decoder_start_token_id=hindi_tag_id,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    # Final cleanup of any tags\n",
        "    clean_text = decoded_text.replace(\"<2hi>\", \"\").strip()\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "v6sDKM1yb4fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# test_df\n",
        "test_df = pd.read_csv('final_test_data_0.csv')\n",
        "\n",
        "print(f\" {len(test_df)} test sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD11kmeEb-DH",
        "outputId": "6193c7f4-c8c9-4ce4-b85e-17455459d953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1746 test sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "#  Processing\n",
        "print(f\" Processing {len(test_df)} sentences...\")\n",
        "test_df['Model_Output'] = test_df['Noisy'].progress_apply(normalize_asr_strict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npNDUB4YcBLQ",
        "outputId": "8d9232dd-08a4-4315-ea0b-a844771afd1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Processing 1746 sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1746/1746 [07:52<00:00,  3.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer -q"
      ],
      "metadata": {
        "id": "nmTZ2LBEcM0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from jiwer import wer, cer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def run_clean_final_report(df):\n",
        "    # Converting the target and predicted text into lists\n",
        "    targets = df['Target'].astype(str).tolist()\n",
        "    preds = df['Model_Output'].astype(str).tolist()\n",
        "\n",
        "    word_overlaps = []\n",
        "    style_mismatches = 0\n",
        "\n",
        "    print(f\"Final Evaluation on {len(df)} samples...\")\n",
        "\n",
        "    for t, p in tqdm(zip(targets, preds), total=len(targets)):\n",
        "        # 1. Word Match (Spelling & Vocabulary Focus)\n",
        "        # Comparing words while ignoring punctuation and symbols\n",
        "        t_w = set(re.sub(r'[^\\u0900-\\u097F\\s]', '', t).split())\n",
        "        p_w = set(re.sub(r'[^\\u0900-\\u097F\\s]', '', p).split())\n",
        "\n",
        "        if t_w:\n",
        "            word_overlaps.append(len(t_w.intersection(p_w)) / len(t_w))\n",
        "\n",
        "        # 2. Finding cases where words are correct but formatting (like spaces) differs\n",
        "        t_clean = \"\".join(re.findall(r'[\\u0900-\\u097F0-9]', t))\n",
        "        p_clean = \"\".join(re.findall(r'[\\u0900-\\u097F0-9]', p))\n",
        "        if t_clean == p_clean and t != p:\n",
        "            style_mismatches += 1\n",
        "\n",
        "    # Calculating Final scores\n",
        "    final_wer = wer(targets, preds)\n",
        "    final_cer = cer(targets, preds)\n",
        "    final_word_acc = np.mean(word_overlaps) * 100\n",
        "\n",
        "    # Displaying the final performance report\n",
        "    print(\"\\n\" + \"═\"*60)\n",
        "    print(\" FINAL ASR NORMALIZATION REPORT\")\n",
        "    print(\"═\"*60)\n",
        "    print(f\" Word-Level Accuracy (ignores punctuation to check contextual correction): {final_word_acc:.2f}%\")\n",
        "    print(f\" Word Error Rate (WER):           {final_wer:.4f}\")\n",
        "    print(f\" Character Error Rate (CER):      {final_cer:.4f}\")\n",
        "    print(f\" Formatting Only Mismatches:      {style_mismatches} sentences\")\n",
        "    print(\"═\"*60)\n",
        "    print(\"\\n NOTE: Word-Level Accuracy measures contextual word identification.\")\n",
        "    print(\" NOTE: Formatting mismatches do not affect the linguistic quality.\")\n",
        "\n",
        "    return {\n",
        "        \"Word_Acc\": final_word_acc,\n",
        "        \"WER\": final_wer,\n",
        "        \"CER\": final_cer,\n",
        "        \"Style_Issues\": style_mismatches\n",
        "    }\n",
        "\n",
        "# EXECUTE\n",
        "final_metrics = run_clean_final_report(test_df.rename(columns={'Clean': 'Target'}))"
      ],
      "metadata": {
        "id": "vzaI2SEVcQct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "id": "qzrVAjXIcROj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub"
      ],
      "metadata": {
        "id": "xOdm81q3cWEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "# FUNCTION 1: Manual Text Normalization\n",
        "def manual_text_normalize(noisy_text):\n",
        "    if not noisy_text.strip():\n",
        "        return \"Please enter some text.\"\n",
        "    # Calling inference function\n",
        "    return quick_test_inference(noisy_text)\n",
        "\n",
        "# FUNCTION 2: Audio Pipeline\n",
        "def process_audio_pipeline(audio_path):\n",
        "    if audio_path is None or not os.path.exists(audio_path):\n",
        "        return \"Audio signal not detected.\", \"\"\n",
        "\n",
        "    recognizer = sr.Recognizer()\n",
        "    unique_wav = f\"temp_{uuid.uuid4().hex}.wav\"\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        audio.export(unique_wav, format=\"wav\")\n",
        "\n",
        "        with sr.AudioFile(unique_wav) as source:\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio_data = recognizer.record(source)\n",
        "            raw_text = recognizer.recognize_google(audio_data, language=\"hi-IN\")\n",
        "\n",
        "        if raw_text.strip():\n",
        "            # Using your model function\n",
        "            clean_text = quick_test_inference(raw_text)\n",
        "            return raw_text, clean_text\n",
        "        else:\n",
        "            return \"Speech not recognized.\", \"Please speak clearly.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"Please try again.\"\n",
        "    finally:\n",
        "        if os.path.exists(unique_wav):\n",
        "            os.remove(unique_wav)\n",
        "\n",
        "# GRADIO INTERFACE\n",
        "#  light theme using custom CSS and Soft theme\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
        "    gr.Markdown(\"Raw ASR Normalizer\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # TAB 1: Speech to Text\n",
        "        with gr.TabItem(\"Audio Normalizer\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    audio_input = gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\", label=\"Record/Upload\")\n",
        "                    btn_audio = gr.Button(\"Process Audio\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    raw_out = gr.Textbox(label=\"Raw ASR Output\", lines=3)\n",
        "                    clean_out = gr.Textbox(label=\"Corrected Text\", lines=3)\n",
        "\n",
        "            btn_audio.click(process_audio_pipeline, inputs=audio_input, outputs=[raw_out, clean_out])\n",
        "\n",
        "        # TAB 2: Manual Text Input\n",
        "        with gr.TabItem(\"Direct Text Input\"):\n",
        "            gr.Markdown(\"### Enter Noisy Hindi Text manually to clean it\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    text_input = gr.Textbox(label=\"Input Noisy Text\", placeholder=\"e.g., में वहा जा रहा हु...\", lines=5)\n",
        "                    btn_text = gr.Button(\"Normalize Text\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    text_output = gr.Textbox(label=\"Corrected Output\", lines=5)\n",
        "\n",
        "            btn_text.click(manual_text_normalize, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"Built with IndicBART & Google ASR Engine\")\n",
        "\n",
        "# Launch\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "aTbKypyBcYeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'demo' in locals():\n",
        "    demo.close()"
      ],
      "metadata": {
        "id": "pnsu9_RFcc9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}